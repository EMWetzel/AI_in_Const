{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZTxtNSBbeGzFQ3WoMru8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EMWetzel/AI_in_Const/blob/main/QA_Gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following exercise will use open-source, Large Language Models to read a text document, tokenize it, and create questions with answers. Note: There are some commented out lines that would export a Word Doc with the Q & A.\n",
        "\n",
        "\n",
        "**Before we get started, change your runtime option to a GPU. Although this isn't super compute heavy, it will help speed it up a bit.**"
      ],
      "metadata": {
        "id": "3R283hOu2HK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will need to import the proper libraries.\n",
        "\n",
        "\"Transformers\" is a PyTorch library that is designed for LLM/NLP. It has models, tokenizers, model weights, etc.\n"
      ],
      "metadata": {
        "id": "or92kXYi3bEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6DR7a4_2EtZ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
        "#If you want to run from VS Code to export a Word Doc, \"pip install python-docx\" and \"from docx import Document\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will load the specific models we will use for both the question and answer tokenization and generation."
      ],
      "metadata": {
        "id": "p1KskpOK5MTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_generation_model_name = \"valhalla/t5-small-qg-hl\"\n",
        "answer_generation_model_name = \"t5-base\"\n",
        "\n",
        "question_tokenizer = T5Tokenizer.from_pretrained(question_generation_model_name)\n",
        "question_model = T5ForConditionalGeneration.from_pretrained(question_generation_model_name)\n",
        "\n",
        "answer_tokenizer = T5Tokenizer.from_pretrained(answer_generation_model_name)\n",
        "answer_model = T5ForConditionalGeneration.from_pretrained(answer_generation_model_name)\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "pbochWuy5mwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define all of our functions. Remember, the order we define these in this section doesn't matter, as long as we call the function in the correct order."
      ],
      "metadata": {
        "id": "VEPxuA2K7UsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qa_pairs(text):\n",
        "    inputs = question_tokenizer.encode(\"highlight: \" + text + \" </s>\", return_tensors=\"pt\")\n",
        "    outputs = question_model.generate(inputs, max_length=150, num_return_sequences=5, do_sample=True)\n",
        "\n",
        "    questions = [question_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "    qa_pairs = []\n",
        "    for question in questions:\n",
        "        input_text = \"question: \" + question + \" context: \" + text + \" </s>\"\n",
        "        inputs = answer_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "        outputs = answer_model.generate(inputs, max_length=150, num_return_sequences=1, do_sample=True)\n",
        "\n",
        "        answer = answer_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        qa_pairs.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "def print_qa_pairs(qa_pairs):\n",
        "    print(\"Generated Q&A Pairs:\\n\")\n",
        "    for i, pair in enumerate(qa_pairs):\n",
        "        print(f'Q{i+1}: {pair[\"question\"]}')\n",
        "        print(f'A{i+1}: {pair[\"answer\"]}\\n')\n",
        "\n",
        "#This does not get used, unless you run in VS Code and want to export a Word Doc\n",
        "def save_to_word(qa_pairs, output_file):\n",
        "    document = Document()\n",
        "    document.add_heading('Generated Q&A Pairs', 0)\n",
        "\n",
        "    for i, pair in enumerate(qa_pairs):\n",
        "        document.add_heading(f'Q{i+1}: {pair[\"question\"]}', level=1)\n",
        "        document.add_paragraph(pair[\"answer\"])\n",
        "\n",
        "    document.save(output_file)\n",
        "\n",
        "def read_text_from_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "        print(\"File read successfully!\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def tokenize_text(text):\n",
        "  tokens = question_tokenizer.tokenize(text)\n",
        "  print(\"Tokenized text:\", tokens)"
      ],
      "metadata": {
        "id": "ivjOmogP6jET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will begin calling the functions. The first function is to read the text file, including it's file path. Note in the function definition that I have added a little \"print\" test to make sure that this is read properly. If it is successful, the output of running this will be a notification.\n",
        "\n",
        "For this to work, we need to import the text document we wish to run.\n",
        "\n",
        "Click on the document icon on the left, load the text document, click on the three dots, and copy the file path. Place it into the input_file between the quotes."
      ],
      "metadata": {
        "id": "pQX59Twq7oHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/TestText.txt\"  # Replace with your .txt file path\n",
        "text = read_text_from_file(input_file)"
      ],
      "metadata": {
        "id": "_dxp1vER8NGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how the tokenizer model works. The below code will tokenize and then print the tokens from the attached document."
      ],
      "metadata": {
        "id": "FR6l17cM-vjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text(text)"
      ],
      "metadata": {
        "id": "eSq1BV3s-9dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our last step is to generate the Q&A pairs and save them to a Word Doc."
      ],
      "metadata": {
        "id": "UeG5VA9S_tLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "qa_pairs = generate_qa_pairs(text)\n",
        "\n",
        "print_qa_pairs(qa_pairs)\n",
        "\n",
        "#If you would rather export to VS Code and print a Word Doc, uncomment and run this\n",
        "\n",
        "#output_file = \"qa_pairs.docx\"\n",
        "#save_to_word(qa_pairs, output_file)\n",
        "#print(f\"Q&A pairs saved to {output_file}\")"
      ],
      "metadata": {
        "id": "TSpVbxeF_zcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shouldn't be a surprise if the Q&A doesn't make a lot of sense because this model has never been trained on this content. However, Q&A generation is a vital step in training a text-based model."
      ],
      "metadata": {
        "id": "NQc2_GyKCM13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EP9tAufM7S9P"
      }
    }
  ]
}